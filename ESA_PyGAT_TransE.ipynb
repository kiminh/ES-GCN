{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESA-PyGAT-TransE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "20dMD50zzwzr",
        "colab_type": "code",
        "outputId": "678d5814-2430-4a02-b728-05c5dca6a899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Synchronize to google drive, define the root path\n",
        "import os\n",
        "\n",
        "google_colab  = True\n",
        "if google_colab == True:\n",
        "  #This statement used to pointing the google drive storage \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  root_path = 'gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "  #This statement is purposed to import library from the Drive\n",
        "  os.chdir('gdrive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxsAJHB8Fyeq",
        "colab_type": "code",
        "outputId": "c37188af-fbc8-4914-a823-21a708a9590d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install rdflib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 2.9MB/s \n",
            "\u001b[?25hCollecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib) (1.12.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.0 rdflib-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h64DnNkRRt5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path as path\n",
        "import re\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import rdflib as rdf\n",
        "import gzip\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import scipy\n",
        "from collections import OrderedDict\n",
        "\n",
        "class OrderedGraph(nx.MultiDiGraph):\n",
        "  node_dict_factory = OrderedDict\n",
        "\n",
        "def parser(f):\n",
        "    triples = list()\n",
        "    for i, triple in enumerate(f):\n",
        "        # extract subject\n",
        "        sub = triple.strip().replace(\"<\", \"\").split(\">\")[0]\n",
        "        sub = sub[sub.rfind(\"/\")+1:]\n",
        "        # extract content from \"content\"\n",
        "        if \"\\\"\" in sub:\n",
        "            pattern = re.compile('\"(.*)\"')\n",
        "            try:\n",
        "                sub_new = pattern.findall(sub)[0]\n",
        "            except IndexError:\n",
        "                # like \"United States/Australian victory\"\n",
        "                sub = sub.replace(\"\\\"\", \"\").strip()\n",
        "                sub_new = sub\n",
        "        # extract content from \":content\"\n",
        "        elif \":\" in sub:\n",
        "            pattern = re.compile(':(.*)')\n",
        "            sub_new = pattern.findall(sub)[0]\n",
        "        else:\n",
        "            sub_new = sub\n",
        "        sub_new = sub_new.replace(\" \", \"\")\n",
        "\n",
        "        # extract object\n",
        "        obj = triple.strip().replace(\"<\", \"\").split(\">\")[2]\n",
        "        # fix extract content form \"content\\\"\n",
        "        if obj.rfind(\"/\")+1 == len(obj):\n",
        "            obj = obj[:-1]\n",
        "        obj = obj[obj.rfind(\"/\")+1:]\n",
        "        # extract content from \"content\"\n",
        "        if \"\\\"\" in obj:\n",
        "            pattern = re.compile('\"(.*)\"')\n",
        "            try:\n",
        "                obj_new = pattern.findall(obj)[0]\n",
        "            except IndexError:\n",
        "                # like \"United States/Australian victory\"\n",
        "                obj = obj.replace(\"\\\"\", \"\").strip()\n",
        "                obj_new = obj\n",
        "        # extract content from \":content\"\n",
        "        elif \":\" in obj:\n",
        "            pattern = re.compile(':(.*)')\n",
        "            obj_new = pattern.findall(obj)[0]\n",
        "        else:\n",
        "            obj_new = obj\n",
        "        obj_new = obj_new.replace(\" \", \"\")\n",
        "        if obj_new == \"\":\n",
        "            obj_new = \"UNK\"\n",
        "        \n",
        "        # extract predicate\n",
        "        pred = triple.strip().replace(\"<\", \"\").split(\">\")[1]\n",
        "        pred = pred[pred.rfind(\"/\")+1:]\n",
        "        if \"#\" in pred:\n",
        "            pattern = re.compile('#(.*)')\n",
        "            pred_new = pattern.findall(pred)[0]\n",
        "        elif \":\" in pred:\n",
        "            pattern = re.compile(':(.*)')\n",
        "            pred_new = pattern.findall(pred)[0]\n",
        "        else:\n",
        "            pred_new = pred\n",
        "        pred_new = pred_new.replace(\" \", \"\")\n",
        "        if not (sub_new == \"\" or pred_new == \"\" or obj_new == \"\"):\n",
        "            triple_tuple = (i, sub, pred, obj, sub_new.replace(\" \", \"\"), pred_new.replace(\" \", \"\"), obj_new.replace(\" \", \"\"))\n",
        "            triples.append(triple_tuple)\n",
        "        else:\n",
        "            print(triple)\n",
        "    return triples\n",
        "\n",
        "# prepare data for per entity\n",
        "def prepare_data(db_path, num):\n",
        "    with open(path.join(db_path, \n",
        "        \"{}\".format(num), \n",
        "        \"{}_desc.nt\".format(num)),\n",
        "        encoding=\"utf8\") as f:\n",
        "        triples = parser(f)\n",
        "    return triples\n",
        "\n",
        "# prepeare label for per label\n",
        "def prepare_label(db_path, num, top_n, file_n):\n",
        "    per_entity_label_dict = {}\n",
        "    for i in range(file_n):\n",
        "        with open(path.join(db_path, \n",
        "            \"{}\".format(num), \n",
        "            \"{}_gold_top{}_{}.nt\".format(num, top_n, i).format(num)),\n",
        "            encoding=\"utf8\") as f:\n",
        "            labels  = parser(f)\n",
        "            for _, _, _, _, _, pred_new, obj_new in labels:\n",
        "                counter(per_entity_label_dict, \"{}++$++{}\".format(pred_new, obj_new))\n",
        "    return per_entity_label_dict\n",
        "\n",
        "# dict counter\n",
        "def counter(cur_dict, word):\n",
        "    if word in cur_dict:\n",
        "        cur_dict[word] += 1\n",
        "    else:\n",
        "        cur_dict[word] = 1\n",
        "\n",
        "# prepare data graph for per entity\n",
        "def build_graph(db_path, num):\n",
        "    G = nx.Graph()\n",
        "    with RDFReader(path.join(db_path, \"{}\".format(num), \"{}_desc.nt\".format(num))) as reader:\n",
        "      relations = reader.relationList()\n",
        "      subjects = reader.subjectSet()\n",
        "      objects = reader.objectSet()\n",
        "      relations_dict = {rel: i+1 for i, rel in enumerate(list(relations))}\n",
        "      relations_dict.update({'UNK':0})\n",
        "      \n",
        "      triples=list()\n",
        "      nodes_dict={}\n",
        "      nodes_dict.update({'UNK':0})\n",
        "      for (s, p, o) in reader.triples():\n",
        "        if s not in nodes_dict:\n",
        "          nodes_dict[s] = len(nodes_dict)\n",
        "        \n",
        "        if o not in nodes_dict:\n",
        "          nodes_dict[o] = len(nodes_dict)\n",
        "        else:\n",
        "          n = 1\n",
        "          \n",
        "          new_status = True\n",
        "          while new_status: \n",
        "            o_new = str(o) + '_{}'.format(n)\n",
        "            if o_new not in nodes_dict:\n",
        "              new_status = False\n",
        "            n +=1\n",
        "          nodes_dict[o_new] = len(nodes_dict)\n",
        "          o = o_new\n",
        "        triple_tuple = (s, p, o)\n",
        "        triples.append(triple_tuple)\n",
        "      nodes_index = [nodes_dict[node] for node in nodes_dict.keys()]\n",
        "      \n",
        "      for (s, p, o) in triples:\n",
        "        G.add_node(nodes_dict[s])\n",
        "        G.add_node(nodes_dict[o])\n",
        "        G.add_edge(nodes_dict[s], nodes_dict[o])\n",
        "    return G, triples  \n",
        "\n",
        "def process_data(db_name, db_start, db_end, top_n=10, file_n=6):\n",
        "    if db_name == \"dbpedia\":\n",
        "        db_path = path.join(path.join(\"data\"), \"dbpedia\")\n",
        "    elif db_name == \"lmdb\":\n",
        "        db_path = path.join(path.join(\"data\"), \"lmdb\")\n",
        "    else:\n",
        "        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "\n",
        "    data, data_for_transE = [], []\n",
        "    label = []\n",
        "    data_graph = []\n",
        "    for i in range(db_start[0], db_end[0]):\n",
        "        graph, triples = build_graph(db_path, i)\n",
        "        sub_data = []\n",
        "        per_entity_data = prepare_data(db_path, i)\n",
        "        for _, _, _, _, _, pred_new, obj_new in per_entity_data:\n",
        "          sub_data.append([pred_new, obj_new])\n",
        "        if len(sub_data) < graph.number_of_nodes():\n",
        "          n_subdata = graph.number_of_nodes() - len(sub_data)\n",
        "          for n in range(n_subdata):  \n",
        "            sub_data.append(['UNK', 'UNK'])\n",
        "        data.append(sub_data)\n",
        "        data_for_transE.extend([[sub_new, obj_new, pred_new]for _, _, _, _, sub_new, pred_new, obj_new in per_entity_data]) \n",
        "        data_graph.append(graph)\n",
        "    for i in range(db_start[1], db_end[1]):\n",
        "        graph, triples = build_graph(db_path, i)\n",
        "        sub_data = []\n",
        "        per_entity_data = prepare_data(db_path, i)\n",
        "        for _, _, _, _, _, pred_new, obj_new in per_entity_data:\n",
        "          sub_data.append([pred_new, obj_new])\n",
        "        if len(sub_data) < graph.number_of_nodes():\n",
        "          n_subdata = graph.number_of_nodes() - len(sub_data)\n",
        "          for n in range(n_subdata):  \n",
        "            sub_data.append(['UNK', 'UNK'])\n",
        "        data.append(sub_data)\n",
        "        data_for_transE.extend([[sub_new, obj_new, pred_new]for _, _, _, _, sub_new, pred_new, obj_new in per_entity_data])\n",
        "        data_graph.append(graph)\n",
        "\n",
        "    for i in range(db_start[0], db_end[0]): \n",
        "        per_entity_label_dict = prepare_label(db_path, i, top_n=top_n, file_n=file_n)\n",
        "        label.append(per_entity_label_dict)\n",
        "\n",
        "    for i in range(db_start[1], db_end[1]): \n",
        "        per_entity_label_dict = prepare_label(db_path, i, top_n=top_n, file_n=file_n)\n",
        "        label.append(per_entity_label_dict)\n",
        "        \n",
        "    # entity dict\n",
        "    entity2ix = {}\n",
        "    entity2ix['UNK'] = 0\n",
        "    for sub_new, obj_new, _ in data_for_transE:\n",
        "        if sub_new not in entity2ix:\n",
        "            entity2ix[sub_new] = len(entity2ix)\n",
        "        if obj_new not in entity2ix:\n",
        "            entity2ix[obj_new] = len(entity2ix)\n",
        "\n",
        "    # pred dict\n",
        "    pred2ix = {}  \n",
        "    pred2ix['UNK']= 0\n",
        "    for _, _, pred_new in data_for_transE:\n",
        "        if pred_new not in pred2ix:\n",
        "            pred2ix[pred_new] = len(pred2ix)\n",
        "      \n",
        "    return data, data_for_transE, label, entity2ix, pred2ix, data_graph\n",
        "\n",
        "def gen_data_transE(db_name, entity_to_ix, pred_to_ix, data_for_transE):\n",
        "    # make dir\n",
        "    if db_name == \"dbpedia\":\n",
        "        directory = path.join(path.join(\"data\"), \"dbpedia_transE\")\n",
        "    elif db_name == \"lmdb\":\n",
        "        directory = path.join(path.join(\"data\"), \"lmdb_transE\")\n",
        "    else:\n",
        "        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "    if not path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    with open(path.join(directory, \"entity2id.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        dict_sorted =  sorted(entity_to_ix.items(), key = lambda x:x[1], reverse = False)\n",
        "        f.write(\"{}\\n\".format(len(entity_to_ix)))\n",
        "        for entity in dict_sorted:\n",
        "            f.write(\"{}\\t{}\\n\".format(entity[0], entity[1]))\n",
        "\n",
        "    with open(path.join(directory, \"relation2id.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        dict_sorted =  sorted(pred_to_ix.items(), key = lambda x:x[1], reverse = False)\n",
        "        f.write(\"{}\\n\".format(len(pred_to_ix)))\n",
        "        for relation in dict_sorted:\n",
        "            f.write(\"{}\\t{}\\n\".format(relation[0], relation[1]))\n",
        "\n",
        "    with open(path.join(directory, \"train2id.txt\"), \"w\", encoding=\"utf-8\") as f:    \n",
        "        # train2id \n",
        "        f.write(\"{}\\n\".format(len(data_for_transE)))\n",
        "        for [sub, obj, pred] in data_for_transE:\n",
        "            f.write(\"{}\\t{}\\t{}\\n\".format(entity_to_ix[sub], entity_to_ix[obj], pred_to_ix[pred]))\n",
        "\n",
        "# load transE\n",
        "def build_dict(f_path):\n",
        "    word2ix = {}\n",
        "    with open(f_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for _, pair in enumerate(f):\n",
        "            try:\n",
        "                temp = pair.strip().split(\"\\t\")\n",
        "                word2ix[temp[0]] = int(temp[1])\n",
        "            except:\n",
        "                print(temp)\n",
        "    return word2ix\n",
        "\n",
        "def build_vec(word2ix, word_embedding):\n",
        "    word2vec = {}\n",
        "    for word in word2ix:\n",
        "        word2vec[word] = word_embedding[int(word2ix[word])]\n",
        "    return word2vec\n",
        "\n",
        "def load_transE(db_name):\n",
        "    if db_name == \"dbpedia\":\n",
        "        directory = path.join(path.join(\"data\"), \"dbpedia_transE\")\n",
        "    elif db_name == \"lmdb\":\n",
        "        directory = path.join(path.join(\"data\"), \"lmdb_transE\")\n",
        "    else:\n",
        "        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "\n",
        "    entity2ix = build_dict(path.join(directory, \"entity2id.txt\"))\n",
        "    pred2ix = build_dict(path.join(directory, \"relation2id.txt\"))\n",
        "\n",
        "    embedding = np.load(path.join(directory, \"transE_vec.npz\"))\n",
        "    entity_embedding = embedding[\"ent_embedding\"]\n",
        "    pred_embedding = embedding[\"rel_embedding\"]\n",
        "\n",
        "    entity2vec = build_vec(entity2ix, entity_embedding)\n",
        "    pred2vec = build_vec(pred2ix, pred_embedding)\n",
        "    return entity2vec, pred2vec, entity2ix, pred2ix\n",
        "\n",
        "def tensor_from_data(entity2vec, pred2ix, data):\n",
        "    pred_list, obj_list = [], []\n",
        "    for pred, obj in data:\n",
        "        pred_list.append(pred2ix[pred])\n",
        "        obj_list.append(entity2vec[obj])\n",
        "    pred_tensor = torch.tensor(pred_list).view(-1, 1)\n",
        "    obj_tensor = torch.tensor(obj_list).unsqueeze(1)\n",
        "    return pred_tensor, obj_tensor\n",
        "\n",
        "def tensor_from_weight(tensor_size, data, label):\n",
        "    weight_tensor = torch.zeros(tensor_size)\n",
        "    for label_word in label:\n",
        "        order = -1\n",
        "        for pred, obj in data:\n",
        "            order += 1\n",
        "            data_word = \"{}++$++{}\".format(pred, obj)\n",
        "            if label_word == data_word:\n",
        "                weight_tensor[order] += label[label_word]\n",
        "                break\n",
        "    return weight_tensor / torch.sum(weight_tensor)\n",
        "\n",
        "# split data for cross validation\n",
        "def split_data(base, num, data, label, data_graph):\n",
        "    start = num * base\n",
        "    end = (num + 1) * base\n",
        "    test_data = data[start:end]\n",
        "    test_label = label[start:end]\n",
        "    test_graph = data_graph[start:end]\n",
        "    train_data, train_label = [], []\n",
        "    train_graph = []\n",
        "    for i, adjacency in enumerate(data_graph):\n",
        "      if i not in range(start, end):\n",
        "        train_graph.append(adjacency)\n",
        "    for i, triples in enumerate(data):\n",
        "        if i not in range(start, end):\n",
        "            train_data.append(triples)\n",
        "    for i, triples in enumerate(label):\n",
        "        if i not in range(start, end):\n",
        "            train_label.append(triples)\n",
        "    return train_data, train_label, train_graph, test_data, test_label, test_graph\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   # dbpedia\n",
        "   #  1 - 100, 141 - 165\n",
        "   #data, data_for_transE, label, entity2ix, pred2ix = process_data(\"dbpedia\", [1, 141], [101, 166])\n",
        "   #gen_data_transE(\"dbpedia\", entity2ix, pred2ix, data_for_transE)\n",
        "   # lmdb\n",
        "   # 101 - 140, 166 - 176\n",
        "   #data, data_for_transE, label, entity2ix, pred2ix = process_data(\"lmdb\", [101, 166], [141, 176])\n",
        "   #gen_data_transE(\"lmdb\", entity2ix, pred2ix, data_for_transE)\n",
        "   None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1UuC02A8ES3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "class RDFReader:\n",
        "    __graph = None\n",
        "    __freq = {}\n",
        "\n",
        "    def __init__(self, file):\n",
        "\n",
        "        self.__graph = rdf.Graph()\n",
        "\n",
        "        self.__graph.parse(file, format='nt')\n",
        "\n",
        "        # See http://rdflib.readthedocs.io for the rdflib documentation\n",
        "\n",
        "        self.__freq = Counter(self.__graph.predicates())\n",
        "\n",
        "        #print(\"Graph loaded, frequencies counted.\")\n",
        "\n",
        "    def triples(self, relation=None):\n",
        "        for s, p, o in self.__graph.triples((None, relation, None)):\n",
        "            yield s, p, o\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.__graph.destroy(\"store\")\n",
        "        self.__graph.close(True)\n",
        "\n",
        "    def subjectSet(self):\n",
        "        return set(self.__graph.subjects())\n",
        "\n",
        "    def objectSet(self):\n",
        "        return set(self.__graph.objects())\n",
        "\n",
        "    def relationList(self):\n",
        "        \"\"\"\n",
        "        Returns a list of relations, ordered descending by frequenecy\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        res = list(set(self.__graph.predicates()))\n",
        "        res.sort(key=lambda rel: - self.freq(rel))\n",
        "        return res\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.__graph)\n",
        "\n",
        "    def freq(self, relation):\n",
        "        \"\"\"\n",
        "        The frequency of this relation (how many distinct triples does it occur in?)\n",
        "        :param relation:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if relation not in self.__freq:\n",
        "            return 0\n",
        "        return self.__freq[relation]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsw08MAxhIZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data, data_for_transE, label, entity2ix, pred2ix, graph = process_data(\"dbpedia\", [1, 141], [101, 166])\n",
        "#gen_data_transE(\"dbpedia\", entity2ix, pred2ix, data_for_transE)\n",
        "#train_data, train_label, train_graph, _, _, _ = split_data(base, i, data, label, data_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwl7Lm3Y4Xup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#train_data,test_data = train_test_split(data_for_transE,test_size=0.2)\n",
        "#test_data,valid_data = train_test_split(test_data,test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSunhjye5bak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def gen_data_transE(db_name, entity_to_ix, pred_to_ix, data_for_transE):\n",
        "#    # make dir\n",
        "#    if db_name == \"dbpedia\":\n",
        "#        directory = path.join(path.join(\"data\"), \"dbpedia_transE\")\n",
        "#    elif db_name == \"lmdb\":\n",
        "#        directory = path.join(path.join(\"data\"), \"lmdb_transE\")\n",
        "#    else:\n",
        "#        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "#    if not path.exists(directory):\n",
        "#        os.makedirs(directory)\n",
        "\n",
        "#    with open(path.join(directory, \"valid2id_.txt\"), \"w\", encoding=\"utf-8\") as f:    \n",
        "        # train2id \n",
        "#        f.write(\"{}\\n\".format(len(data_for_transE)))\n",
        "#        for [sub, obj, pred] in data_for_transE:\n",
        "#            f.write(\"{}\\t{}\\t{}\\n\".format(entity_to_ix[sub], entity_to_ix[obj], pred_to_ix[pred]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnuyWK9M53Bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gen_data_transE(\"dbpedia\", entity2ix, pred2ix, valid_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZqjYwfEhTN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data, data_for_transE, label, entity2ix, pred2ix, graph = process_data(\"lmdb\", [101, 166], [141, 176])\n",
        "#train_data,test_data = train_test_split(data_for_transE,test_size=0.2)\n",
        "#test_data,valid_data = train_test_split(test_data,test_size=0.5)\n",
        "#gen_data_transE(\"lmdb\", entity2ix, pred2ix, data_for_transE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klx8_GwX6bxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#gen_data_transE(\"lmdb\", entity2ix, pred2ix, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1AiOhiO-mkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pygat.models import GAT, SpGAT\n",
        "from pygat.utils import normalize_adj, normalize_features\n",
        "import scipy.sparse as spT\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class ESGCN(nn.Module):\n",
        "    def __init__(self, pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device):\n",
        "        super(ESGCN, self).__init__()\n",
        "        self.pred2ix_size = pred2ix_size\n",
        "        self.pred_embedding_dim = pred_embedding_dim\n",
        "        self.transE_dim = transE_dim\n",
        "        self.input_size = self.transE_dim + self.pred_embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        print('hidden_size', hidden_size)\n",
        "        self.embedding = nn.Embedding(self.pred2ix_size, self.pred_embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True)\n",
        "        self.gat = GAT(nfeat=400, nhid=32, nclass=1, dropout=0.5, nheads=8, alpha=0.2)\n",
        "        self.device = device\n",
        "        self.initial_hidden = self._init_hidden()\n",
        "        \n",
        "    def forward(self, input_tensor, G):\n",
        "        # bi-lstm\n",
        "        pred_embedded = self.embedding(input_tensor[0])\n",
        "        obj_embedded = input_tensor[1]\n",
        "        embedded = torch.cat((pred_embedded, obj_embedded), 2)\n",
        "        lstm_out, (hidden_state, cell_state) = self.lstm(embedded, self.initial_hidden)\n",
        "        #lstm_out = lstm_out.permute(1, 0, 2)\n",
        "        lstm_out = torch.flatten(lstm_out, start_dim=1)\n",
        "        #print('lstm_out', lstm_out)\n",
        "        #lstm_out = lstm_out.view(lstm_out.shape[0], -1)\n",
        "        \n",
        "        #pygcn\n",
        "        adj = nx.adjacency_matrix(G)\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "        #print('adj', adj)\n",
        "\n",
        "        features = normalize_features(lstm_out.detach().numpy())\n",
        "        features = torch.FloatTensor(np.array(features))\n",
        "        \n",
        "        logits = self.gat(features, adj)\n",
        "        #logp = F.log_softmax(logits, 1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def _init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_size, device=self.device), \n",
        "            torch.randn(2, 1, self.hidden_size, device=self.device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E160725LCClO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "1c0bd2b5-8305-4113-eaff-bdb0cc7aa1f8"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "import os.path as path\n",
        "import sys\n",
        "import argparse\n",
        "ROOTDIR = path.dirname(os.getcwd())\n",
        "DATADIR = path.join(\"data\")\n",
        "sys.path.append(DATADIR)\n",
        "import utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, g, features, labels):\n",
        "    model.eval()\n",
        "    with th.no_grad():\n",
        "        logits = model(g, features)\n",
        "        labels = labels\n",
        "        _, indices = th.max(logits, dim=1)\n",
        "        correct = th.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "def train(esgcn, data, label, criterion, optimizer, n_epoch, save_every, directory, device, clip, entity2vec, pred2ix, regularization, adj):\n",
        "    if not path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    print('n epoch', n_epoch)\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i in range(len(data)):\n",
        "            esgcn.zero_grad()\n",
        "            pred_tensor, obj_tensor = tensor_from_data(entity2vec, pred2ix, data[i])\n",
        "            input_tensor = [pred_tensor.to(device), obj_tensor.to(device)]\n",
        "            weight_tensor = tensor_from_weight(len(data[i]), data[i], label[i]).to(device)\n",
        "            atten_weight = esgcn(input_tensor, adj[i])\n",
        "            # loss\n",
        "            if regularization:\n",
        "                loss = criterion(atten_weight.view(-1), weight_tensor.view(-1)).to(device) + \\\n",
        "                    torch.sum(torch.abs(atten_weight))\n",
        "            else:\n",
        "                loss = criterion(atten_weight.view(-1), weight_tensor.view(-1)).to(device)\n",
        "\n",
        "            # clip gradient\n",
        "            _ = nn.utils.clip_grad_norm_(esgcn.parameters(), clip)\n",
        "\n",
        "            #optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        total_loss = total_loss/len(data)\n",
        "        if epoch % save_every == 0:\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": esgcn.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": total_loss\n",
        "                }, path.join(directory, \"checkpoint_epoch_{}.pt\".format(epoch)))\n",
        "        print(\"epoch: {}\".format(epoch), total_loss)\n",
        "\n",
        "def train_iter(db_name, base, data, label, pred2ix, pred2ix_size, entity2vec, pred_embedding_dim, transe_dim, hidden_size, criterion, clip, lr, n_epoch, save_every, regularization, device, data_graph):\n",
        "    if regularization == True:\n",
        "        print(\"use regularization in training\")\n",
        "    for i in range(5):\n",
        "        train_data, train_label, train_graph, _, _, _ = split_data(base, i, data, label, data_graph)\n",
        "        esgcn = ESGCN(pred2ix_size, pred_embedding_dim, transe_dim, hidden_size, device)\n",
        "        esgcn.to(device)\n",
        "        print(esgcn)\n",
        "        optimizer = optim.Adam(esgcn.parameters(), lr=lr,  weight_decay=5e-4)\n",
        "        directory = os.path.join(os.getcwd(), \"esgcn_checkpoint-{}-{}\".format(db_name, i))\n",
        "        train(esgcn, train_data, train_label, criterion, optimizer, n_epoch, save_every, directory, device, clip, entity2vec, pred2ix, regularization, train_graph)\n",
        "\n",
        "def writer(DB_DIR, skip_i, directory, top_or_rank, output):\n",
        "    with open(path.join(DB_DIR, \n",
        "            \"{}\".format(skip_i+1), \n",
        "            \"{}_desc.nt\".format(skip_i+1)),\n",
        "            encoding=\"utf8\") as fin, \\\n",
        "    open(path.join(directory,\n",
        "            \"{}\".format(skip_i+1),\n",
        "            \"{}_{}.nt\".format(skip_i+1, top_or_rank)),\n",
        "            \"w\", encoding=\"utf8\") as fout:\n",
        "        if top_or_rank == \"top5\" or top_or_rank == \"top10\":\n",
        "            top_list = output.squeeze(0).numpy().tolist()\n",
        "            print('top list', top_list)\n",
        "            for t_num, triple in enumerate(fin):\n",
        "                if t_num in top_list:\n",
        "                    fout.write(triple)\n",
        "        elif top_or_rank == \"rank\":\n",
        "            rank_list = output.squeeze(0).numpy().tolist()\n",
        "            #print(rank_list)\n",
        "            triples = [triple for _, triple in enumerate(fin)]\n",
        "            for rank in rank_list:\n",
        "              try:\n",
        "                  fout.write(triples[rank])\n",
        "              except:\n",
        "                  pass\n",
        "                    \n",
        "    return\n",
        "\n",
        "def generator(DB_NAME, base, data, label, entity2vec, pred2ix, pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device, use_epoch, db_base, DB_DIR, skip_num, data_graph):\n",
        "    directory = path.join(\"data_esa\", DB_NAME)\n",
        "    if not path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    print(\"generating entity summarization results:\") \n",
        "    for num in tqdm(range(5)):\n",
        "        CHECK_DIR = path.join(\"esgcn_checkpoint-{}-{}\".format(DB_NAME, num))\n",
        "        print(\"CHECK_DIR\", CHECK_DIR)\n",
        "        esgcn = ESGCN(pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device)\n",
        "        checkpoint = torch.load(path.join(CHECK_DIR, \"checkpoint_epoch_{}.pt\".format(use_epoch)))\n",
        "        esgcn.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        esgcn.to(device)\n",
        "        for i in range(num*base, (num+1)*base):\n",
        "            print('i', i)\n",
        "            data_i = i - num*base\n",
        "            print('data_i', data_i)\n",
        "            _, _,_, test_data, test_label, test_graph = split_data(base, num, data, label, data_graph)\n",
        "            pred_tensor, obj_tensor = tensor_from_data(entity2vec, pred2ix, test_data[data_i])\n",
        "            input_tensor = [pred_tensor.to(device), obj_tensor.to(device)]\n",
        "            weight_tensor = tensor_from_weight(len(test_data[data_i]), test_data[data_i], test_label[data_i]).to(device)\n",
        "            atten_weight = esgcn(input_tensor, test_graph[data_i])\n",
        "            atten_weight = atten_weight.view(1, -1).cpu()\n",
        "            weight_tensor = weight_tensor.view(1, -1).cpu()\n",
        "            (_, label_top10) = torch.topk(weight_tensor, 10)\n",
        "            (_, output_top10) = torch.topk(atten_weight, 10)\n",
        "            (_, label_top5) = torch.topk(weight_tensor, 5)\n",
        "            (_, output_top5) = torch.topk(atten_weight, 5)\n",
        "            (_, output_rank) = torch.topk(atten_weight, len(test_data[data_i]))\n",
        "            if num == 4:\n",
        "                skip_i = i + skip_num + db_base\n",
        "            else:\n",
        "                skip_i = i + db_base\n",
        "            if not path.exists(path.join(directory, \"{}\".format(skip_i+1))):\n",
        "                os.makedirs(path.join(directory, \"{}\".format(skip_i+1)))\n",
        "            writer(DB_DIR, skip_i, directory, \"top10\", output_top10)\n",
        "            writer(DB_DIR, skip_i, directory, \"top5\", output_top5)\n",
        "            writer(DB_DIR, skip_i, directory, \"rank\", output_rank)\n",
        "\n",
        "def main(DB_NAME, mode, top_n, file_n, transE_dim, pred_embedding_dim, lr, clip, save_every, n_epoch, use_epoch, loss_function, regularization):\n",
        "  if DB_NAME == \"dbpedia\":\n",
        "    print(\"training model on dbpedia\")\n",
        "    DB_START, DB_END = [1, 141], [101, 166]\n",
        "    base = 25\n",
        "    skip_num = 40\n",
        "    db_base = 0\n",
        "  elif DB_NAME == \"lmdb\":\n",
        "    print(\"training model on lmdb\")\n",
        "    DB_START, DB_END = [101, 166], [141, 176]\n",
        "    base = 10\n",
        "    skip_num = 25\n",
        "    db_base = 100\n",
        "  DB_DIR = path.join(DATADIR, DB_NAME)\n",
        "  print(\"DB_DIR\", DB_DIR)\n",
        "\n",
        "  # load data\n",
        "  data, _, label, _, _, data_graph = process_data(DB_NAME, DB_START, DB_END, top_n, file_n)\n",
        "\n",
        "  entity2vec, pred2vec, entity2ix, pred2ix = load_transE(DB_NAME)\n",
        "  pred2ix_size = len(pred2ix)\n",
        "  hidden_size = transE_dim + pred_embedding_dim\n",
        "\n",
        "  # train\n",
        "  ## cuda \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(\"cuda or cpu: {}\".format(device))\n",
        "\n",
        "  ## loss function\n",
        "  if loss_function == \"BCE\":\n",
        "    criterion = torch.nn.BCELoss()\n",
        "  elif loss_function == \"MSE\":\n",
        "    criterion = torch.nn.MSELoss()\n",
        "  else:\n",
        "    print(\"please choose choose the correct loss fucntion\")\n",
        "    sys.exit()\n",
        "  print(\"loss function: {}\".format(loss_function))\n",
        "\n",
        "  if mode == \"train\" or mode == \"all\":\n",
        "    ## training iteration (5-fold cross validation)\n",
        "    train_iter(DB_NAME, base, data, label, pred2ix, pred2ix_size, entity2vec, pred_embedding_dim, transE_dim, hidden_size, criterion, clip, lr, n_epoch, save_every, regularization, device, data_graph)\n",
        "\n",
        "  if mode == \"test\" or mode == \"all\":\n",
        "    #generate\n",
        "    generator(DB_NAME, base, data, label, entity2vec, pred2ix, pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device, use_epoch, db_base, DB_DIR, skip_num, data_graph)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x42OxJVb9PPb",
        "colab_type": "code",
        "outputId": "ff7c4bb9-62eb-4425-db1b-07de666f085b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "main(\"dbpedia\", \"train\", 5, 6, 100,100, 0.01, 50, 2, 50, 24, \"BCE\", False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model on dbpedia\n",
            "DB_DIR data/dbpedia\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IaDuyj3WuPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main(\"dbpedia\", \"test\", 5, 6, 100,100, 0.01, 50, 2, 50, 24, \"BCE\", False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc7nH8Z9RNoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}