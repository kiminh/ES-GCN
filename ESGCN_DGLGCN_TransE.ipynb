{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESA DGL-TransE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "20dMD50zzwzr",
        "colab_type": "code",
        "outputId": "4fcc02f2-f78f-4859-eb65-e6f7da5d20f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Synchronize to google drive, define the root path\n",
        "import os\n",
        "\n",
        "google_colab  = True\n",
        "if google_colab == True:\n",
        "  #This statement used to pointing the google drive storage \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  root_path = 'gdrive/My Drive/Colab Notebooks/'\n",
        "\n",
        "  #This statement is purposed to import library from the Drive\n",
        "  os.chdir('gdrive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxsAJHB8Fyeq",
        "colab_type": "code",
        "outputId": "800729b0-4a23-4bd3-f6ff-842600a534af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install rdflib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 2.8MB/s \n",
            "\u001b[?25hCollecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib) (1.12.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.0 rdflib-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8yR1BFhnPQh",
        "colab_type": "code",
        "outputId": "94e682a3-6468-4843-98a2-7b8a2717c23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "!pip install dgl # For CPU Build\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl import DGLGraph\n",
        "\n",
        "gcn_msg = fn.copy_src(src='h', out='m')\n",
        "gcn_reduce = fn.sum(msg='m', out='h')\n",
        "\n",
        "class NodeApplyModule(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(NodeApplyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, node):\n",
        "        h = self.linear(node.data['h'])\n",
        "        if self.activation is not None:\n",
        "            h = self.activation(h)\n",
        "        return {'h' : h}\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(GCN, self).__init__()\n",
        "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
        "\n",
        "    def forward(self, g, feature):\n",
        "        g.ndata['h'] = feature\n",
        "        g.update_all(gcn_msg, gcn_reduce)\n",
        "        g.apply_nodes(func=self.apply_mod)\n",
        "        return g.ndata.pop('h')\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.gcn1 = GCN(400, 64, F.relu)\n",
        "        self.gcn2 = GCN(64, 1, None)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        x = self.gcn1(g, features)\n",
        "        x = self.gcn2(g, x)\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/3a/34fcdd3ec13cc945db277f87ca8ea3bf320fba8a6c04dc675f257869f45b/dgl-0.4.2-cp36-cp36m-manylinux1_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.17.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.1)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h64DnNkRRt5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import os.path as path\n",
        "import re\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import rdflib as rdf\n",
        "import gzip\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import scipy\n",
        "from collections import OrderedDict\n",
        "\n",
        "class OrderedGraph(nx.MultiDiGraph):\n",
        "  node_dict_factory = OrderedDict\n",
        "\n",
        "def parser(f):\n",
        "    triples = list()\n",
        "    for i, triple in enumerate(f):\n",
        "        # extract subject\n",
        "        sub = triple.strip().replace(\"<\", \"\").split(\">\")[0]\n",
        "        sub = sub[sub.rfind(\"/\")+1:]\n",
        "        # extract content from \"content\"\n",
        "        if \"\\\"\" in sub:\n",
        "            pattern = re.compile('\"(.*)\"')\n",
        "            try:\n",
        "                sub_new = pattern.findall(sub)[0]\n",
        "            except IndexError:\n",
        "                # like \"United States/Australian victory\"\n",
        "                sub = sub.replace(\"\\\"\", \"\").strip()\n",
        "                sub_new = sub\n",
        "        # extract content from \":content\"\n",
        "        elif \":\" in sub:\n",
        "            pattern = re.compile(':(.*)')\n",
        "            sub_new = pattern.findall(sub)[0]\n",
        "        else:\n",
        "            sub_new = sub\n",
        "        sub_new = sub_new.replace(\" \", \"\")\n",
        "\n",
        "        # extract object\n",
        "        obj = triple.strip().replace(\"<\", \"\").split(\">\")[2]\n",
        "        # fix extract content form \"content\\\"\n",
        "        if obj.rfind(\"/\")+1 == len(obj):\n",
        "            obj = obj[:-1]\n",
        "        obj = obj[obj.rfind(\"/\")+1:]\n",
        "        # extract content from \"content\"\n",
        "        if \"\\\"\" in obj:\n",
        "            pattern = re.compile('\"(.*)\"')\n",
        "            try:\n",
        "                obj_new = pattern.findall(obj)[0]\n",
        "            except IndexError:\n",
        "                # like \"United States/Australian victory\"\n",
        "                obj = obj.replace(\"\\\"\", \"\").strip()\n",
        "                obj_new = obj\n",
        "        # extract content from \":content\"\n",
        "        elif \":\" in obj:\n",
        "            pattern = re.compile(':(.*)')\n",
        "            obj_new = pattern.findall(obj)[0]\n",
        "        else:\n",
        "            obj_new = obj\n",
        "        obj_new = obj_new.replace(\" \", \"\")\n",
        "        if obj_new == \"\":\n",
        "            obj_new = \"UNK\"\n",
        "        \n",
        "        # extract predicate\n",
        "        pred = triple.strip().replace(\"<\", \"\").split(\">\")[1]\n",
        "        pred = pred[pred.rfind(\"/\")+1:]\n",
        "        if \"#\" in pred:\n",
        "            pattern = re.compile('#(.*)')\n",
        "            pred_new = pattern.findall(pred)[0]\n",
        "        elif \":\" in pred:\n",
        "            pattern = re.compile(':(.*)')\n",
        "            pred_new = pattern.findall(pred)[0]\n",
        "        else:\n",
        "            pred_new = pred\n",
        "        pred_new = pred_new.replace(\" \", \"\")\n",
        "        if not (sub_new == \"\" or pred_new == \"\" or obj_new == \"\"):\n",
        "            triple_tuple = (i, sub, pred, obj, sub_new.replace(\" \", \"\"), pred_new.replace(\" \", \"\"), obj_new.replace(\" \", \"\"))\n",
        "            triples.append(triple_tuple)\n",
        "        else:\n",
        "            print(triple)\n",
        "    return triples\n",
        "\n",
        "# prepare data for per entity\n",
        "def prepare_data(db_path, num):\n",
        "    with open(path.join(db_path, \n",
        "        \"{}\".format(num), \n",
        "        \"{}_desc.nt\".format(num)),\n",
        "        encoding=\"utf8\") as f:\n",
        "        triples = parser(f)\n",
        "    return triples\n",
        "\n",
        "# prepeare label for per label\n",
        "def prepare_label(db_path, num, top_n, file_n):\n",
        "    per_entity_label_dict = {}\n",
        "    for i in range(file_n):\n",
        "        with open(path.join(db_path, \n",
        "            \"{}\".format(num), \n",
        "            \"{}_gold_top{}_{}.nt\".format(num, top_n, i).format(num)),\n",
        "            encoding=\"utf8\") as f:\n",
        "            labels  = parser(f)\n",
        "            for _, _, _, _, _, pred_new, obj_new in labels:\n",
        "                counter(per_entity_label_dict, \"{}++$++{}\".format(pred_new, obj_new))\n",
        "    return per_entity_label_dict\n",
        "\n",
        "# dict counter\n",
        "def counter(cur_dict, word):\n",
        "    if word in cur_dict:\n",
        "        cur_dict[word] += 1\n",
        "    else:\n",
        "        cur_dict[word] = 1\n",
        "\n",
        "# prepare data graph for per entity\n",
        "def build_graph(db_path, num):\n",
        "    G = dgl.DGLGraph()\n",
        "    with RDFReader(path.join(db_path, \"{}\".format(num), \"{}_desc.nt\".format(num))) as reader:\n",
        "      relations = reader.relationList()\n",
        "      subjects = reader.subjectSet()\n",
        "      objects = reader.objectSet()\n",
        "      relations_dict = {rel: i+1 for i, rel in enumerate(list(relations))}\n",
        "      relations_dict.update({'UNK':0})\n",
        "      \n",
        "      triples=list()\n",
        "      nodes_dict={}\n",
        "      nodes_dict.update({'UNK':0})\n",
        "      for (s, p, o) in reader.triples():\n",
        "        if s not in nodes_dict:\n",
        "          nodes_dict[s] = len(nodes_dict)\n",
        "        \n",
        "        if o not in nodes_dict:\n",
        "          nodes_dict[o] = len(nodes_dict)\n",
        "        else:\n",
        "          n = 1\n",
        "          \n",
        "          new_status = True\n",
        "          while new_status: \n",
        "            o_new = str(o) + '_{}'.format(n)\n",
        "            if o_new not in nodes_dict:\n",
        "              new_status = False\n",
        "            n +=1\n",
        "          nodes_dict[o_new] = len(nodes_dict)\n",
        "          o = o_new\n",
        "        triple_tuple = (s, p, o)\n",
        "        triples.append(triple_tuple)\n",
        "      nodes_index = [nodes_dict[node] for node in nodes_dict.keys()]\n",
        "      G.add_nodes(len(nodes_index))\n",
        "      for (s, p, o) in triples:\n",
        "        G.add_edge(nodes_dict[s], nodes_dict[o])\n",
        "    return G, triples  \n",
        "\n",
        "def process_data(db_name, db_start, db_end, top_n=10, file_n=6):\n",
        "    if db_name == \"dbpedia\":\n",
        "        db_path = path.join(path.join(\"data\"), \"dbpedia\")\n",
        "    elif db_name == \"lmdb\":\n",
        "        db_path = path.join(path.join(\"data\"), \"lmdb\")\n",
        "    else:\n",
        "        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "\n",
        "    data, data_for_transE = [], []\n",
        "    label = []\n",
        "    data_graph = []\n",
        "    for i in range(db_start[0], db_end[0]):\n",
        "        graph, triples = build_graph(db_path, i)\n",
        "        sub_data = []\n",
        "        per_entity_data = prepare_data(db_path, i)\n",
        "        for _, _, _, _, _, pred_new, obj_new in per_entity_data:\n",
        "          sub_data.append([pred_new, obj_new])\n",
        "        if len(sub_data) < graph.number_of_nodes():\n",
        "          n_subdata = graph.number_of_nodes() - len(sub_data)\n",
        "          for n in range(n_subdata):  \n",
        "            sub_data.append(['UNK', 'UNK'])\n",
        "        data.append(sub_data)\n",
        "        data_for_transE.extend([[sub_new, obj_new, pred_new]for _, _, _, _, sub_new, pred_new, obj_new in per_entity_data]) \n",
        "        data_graph.append(graph)\n",
        "    for i in range(db_start[1], db_end[1]):\n",
        "        graph, triples = build_graph(db_path, i)\n",
        "        sub_data = []\n",
        "        per_entity_data = prepare_data(db_path, i)\n",
        "        for _, _, _, _, _, pred_new, obj_new in per_entity_data:\n",
        "          sub_data.append([pred_new, obj_new])\n",
        "        if len(sub_data) < graph.number_of_nodes():\n",
        "          n_subdata = graph.number_of_nodes() - len(sub_data)\n",
        "          for n in range(n_subdata):  \n",
        "            sub_data.append(['UNK', 'UNK'])\n",
        "        data.append(sub_data)\n",
        "        data_for_transE.extend([[sub_new, obj_new, pred_new]for _, _, _, _, sub_new, pred_new, obj_new in per_entity_data])\n",
        "        data_graph.append(graph)\n",
        "\n",
        "    for i in range(db_start[0], db_end[0]): \n",
        "        per_entity_label_dict = prepare_label(db_path, i, top_n=top_n, file_n=file_n)\n",
        "        label.append(per_entity_label_dict)\n",
        "\n",
        "    for i in range(db_start[1], db_end[1]): \n",
        "        per_entity_label_dict = prepare_label(db_path, i, top_n=top_n, file_n=file_n)\n",
        "        label.append(per_entity_label_dict)\n",
        "        \n",
        "    # entity dict\n",
        "    entity2ix = {}\n",
        "    entity2ix['UNK'] = 0\n",
        "    for sub_new, obj_new, _ in data_for_transE:\n",
        "        if sub_new not in entity2ix:\n",
        "            entity2ix[sub_new] = len(entity2ix)\n",
        "        if obj_new not in entity2ix:\n",
        "            entity2ix[obj_new] = len(entity2ix)\n",
        "\n",
        "    # pred dict\n",
        "    pred2ix = {}  \n",
        "    pred2ix['UNK']= 0\n",
        "    for _, _, pred_new in data_for_transE:\n",
        "        if pred_new not in pred2ix:\n",
        "            pred2ix[pred_new] = len(pred2ix)\n",
        "      \n",
        "    return data, data_for_transE, label, entity2ix, pred2ix, data_graph\n",
        "\n",
        "def gen_data_transE(db_name, entity_to_ix, pred_to_ix, data_for_transE):\n",
        "    # make dir\n",
        "    if db_name == \"dbpedia\":\n",
        "        directory = path.join(path.join(\"data\"), \"dbpedia_transE\")\n",
        "    elif db_name == \"lmdb\":\n",
        "        directory = path.join(path.join(\"data\"), \"lmdb_transE\")\n",
        "    else:\n",
        "        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "    if not path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    with open(path.join(directory, \"entity2id.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        dict_sorted =  sorted(entity_to_ix.items(), key = lambda x:x[1], reverse = False)\n",
        "        f.write(\"{}\\n\".format(len(entity_to_ix)))\n",
        "        for entity in dict_sorted:\n",
        "            f.write(\"{}\\t{}\\n\".format(entity[0], entity[1]))\n",
        "\n",
        "    with open(path.join(directory, \"relation2id.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        dict_sorted =  sorted(pred_to_ix.items(), key = lambda x:x[1], reverse = False)\n",
        "        f.write(\"{}\\n\".format(len(pred_to_ix)))\n",
        "        for relation in dict_sorted:\n",
        "            f.write(\"{}\\t{}\\n\".format(relation[0], relation[1]))\n",
        "\n",
        "    with open(path.join(directory, \"train2id.txt\"), \"w\", encoding=\"utf-8\") as f:    \n",
        "        # train2id \n",
        "        f.write(\"{}\\n\".format(len(data_for_transE)))\n",
        "        for [sub, obj, pred] in data_for_transE:\n",
        "            f.write(\"{}\\t{}\\t{}\\n\".format(entity_to_ix[sub], entity_to_ix[obj], pred_to_ix[pred]))\n",
        "\n",
        "# load transE\n",
        "def build_dict(f_path):\n",
        "    word2ix = {}\n",
        "    with open(f_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for _, pair in enumerate(f):\n",
        "            try:\n",
        "                temp = pair.strip().split(\"\\t\")\n",
        "                word2ix[temp[0]] = int(temp[1])\n",
        "            except:\n",
        "                print(temp)\n",
        "    return word2ix\n",
        "\n",
        "def build_vec(word2ix, word_embedding):\n",
        "    word2vec = {}\n",
        "    for word in word2ix:\n",
        "        word2vec[word] = word_embedding[int(word2ix[word])]\n",
        "    return word2vec\n",
        "\n",
        "def load_transE(db_name):\n",
        "    if db_name == \"dbpedia\":\n",
        "        directory = path.join(path.join(\"data\"), \"dbpedia_transE\")\n",
        "    elif db_name == \"lmdb\":\n",
        "        directory = path.join(path.join(\"data\"), \"lmdb_transE\")\n",
        "    else:\n",
        "        raise ValueError(\"The database's name must be dbpedia or lmdb\")\n",
        "\n",
        "    entity2ix = build_dict(path.join(directory, \"entity2id.txt\"))\n",
        "    pred2ix = build_dict(path.join(directory, \"relation2id.txt\"))\n",
        "\n",
        "    embedding = np.load(path.join(directory, \"transE_vec.npz\"))\n",
        "    entity_embedding = embedding[\"ent_embedding\"]\n",
        "    pred_embedding = embedding[\"rel_embedding\"]\n",
        "\n",
        "    entity2vec = build_vec(entity2ix, entity_embedding)\n",
        "    pred2vec = build_vec(pred2ix, pred_embedding)\n",
        "    return entity2vec, pred2vec, entity2ix, pred2ix\n",
        "\n",
        "def tensor_from_data(entity2vec, pred2ix, data):\n",
        "    pred_list, obj_list = [], []\n",
        "    for pred, obj in data:\n",
        "        pred_list.append(pred2ix[pred])\n",
        "        obj_list.append(entity2vec[obj])\n",
        "    pred_tensor = torch.tensor(pred_list).view(-1, 1)\n",
        "    obj_tensor = torch.tensor(obj_list).unsqueeze(1)\n",
        "    return pred_tensor, obj_tensor\n",
        "\n",
        "def tensor_from_weight(tensor_size, data, label):\n",
        "    weight_tensor = torch.zeros(tensor_size)\n",
        "    for label_word in label:\n",
        "        order = -1\n",
        "        for pred, obj in data:\n",
        "            order += 1\n",
        "            data_word = \"{}++$++{}\".format(pred, obj)\n",
        "            if label_word == data_word:\n",
        "                weight_tensor[order] += label[label_word]\n",
        "                break\n",
        "    return weight_tensor / torch.sum(weight_tensor)\n",
        "\n",
        "# split data for cross validation\n",
        "def split_data(base, num, data, label, data_graph):\n",
        "    start = num * base\n",
        "    end = (num + 1) * base\n",
        "    test_data = data[start:end]\n",
        "    test_label = label[start:end]\n",
        "    test_graph = data_graph[start:end]\n",
        "    train_data, train_label = [], []\n",
        "    train_graph = []\n",
        "    for i, adjacency in enumerate(data_graph):\n",
        "      if i not in range(start, end):\n",
        "        train_graph.append(adjacency)\n",
        "    for i, triples in enumerate(data):\n",
        "        if i not in range(start, end):\n",
        "            train_data.append(triples)\n",
        "    for i, triples in enumerate(label):\n",
        "        if i not in range(start, end):\n",
        "            train_label.append(triples)\n",
        "    return train_data, train_label, train_graph, test_data, test_label, test_graph\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   # dbpedia\n",
        "   #  1 - 100, 141 - 165\n",
        "   #data, data_for_transE, label, entity2ix, pred2ix = process_data(\"dbpedia\", [1, 141], [101, 166])\n",
        "   #gen_data_transE(\"dbpedia\", entity2ix, pred2ix, data_for_transE)\n",
        "   # lmdb\n",
        "   # 101 - 140, 166 - 176\n",
        "   #data, data_for_transE, label, entity2ix, pred2ix = process_data(\"lmdb\", [101, 166], [141, 176])\n",
        "   #gen_data_transE(\"lmdb\", entity2ix, pred2ix, data_for_transE)\n",
        "   None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1UuC02A8ES3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "class RDFReader:\n",
        "    __graph = None\n",
        "    __freq = {}\n",
        "\n",
        "    def __init__(self, file):\n",
        "\n",
        "        self.__graph = rdf.Graph()\n",
        "\n",
        "        self.__graph.parse(file, format='nt')\n",
        "\n",
        "        # See http://rdflib.readthedocs.io for the rdflib documentation\n",
        "\n",
        "        self.__freq = Counter(self.__graph.predicates())\n",
        "\n",
        "        #print(\"Graph loaded, frequencies counted.\")\n",
        "\n",
        "    def triples(self, relation=None):\n",
        "        for s, p, o in self.__graph.triples((None, relation, None)):\n",
        "            yield s, p, o\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.__graph.destroy(\"store\")\n",
        "        self.__graph.close(True)\n",
        "\n",
        "    def subjectSet(self):\n",
        "        return set(self.__graph.subjects())\n",
        "\n",
        "    def objectSet(self):\n",
        "        return set(self.__graph.objects())\n",
        "\n",
        "    def relationList(self):\n",
        "        \"\"\"\n",
        "        Returns a list of relations, ordered descending by frequenecy\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        res = list(set(self.__graph.predicates()))\n",
        "        res.sort(key=lambda rel: - self.freq(rel))\n",
        "        return res\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.__graph)\n",
        "\n",
        "    def freq(self, relation):\n",
        "        \"\"\"\n",
        "        The frequency of this relation (how many distinct triples does it occur in?)\n",
        "        :param relation:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if relation not in self.__freq:\n",
        "            return 0\n",
        "        return self.__freq[relation]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1AiOhiO-mkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pygcn.utils import sparse_mx_to_torch_sparse_tensor, normalize \n",
        "import scipy.sparse as spT\n",
        "\n",
        "class ESA(nn.Module):\n",
        "    def __init__(self, pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device, label):\n",
        "        super(ESA, self).__init__()\n",
        "        self.pred2ix_size = pred2ix_size\n",
        "        self.pred_embedding_dim = pred_embedding_dim\n",
        "        self.transE_dim = transE_dim\n",
        "        self.input_size = self.transE_dim + self.pred_embedding_dim\n",
        "        #print('input size', self.input_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        print('hidden_size', hidden_size)\n",
        "        #print('label', len(label))\n",
        "        self.embedding = nn.Embedding(self.pred2ix_size, self.pred_embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True)\n",
        "        self.gcn_dgl = Net()\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, input_tensor, G):\n",
        "        # bi-lstm\n",
        "        pred_embedded = self.embedding(input_tensor[0])\n",
        "        obj_embedded = input_tensor[1]\n",
        "        embedded = torch.cat((pred_embedded, obj_embedded), 2)\n",
        "        lstm_out, (hidden_state, cell_state) = self.lstm(embedded)\n",
        "        lstm_out = torch.flatten(lstm_out, start_dim=1)\n",
        "        features = normalize(lstm_out.detach().numpy())\n",
        "        features = torch.FloatTensor(np.array(features))\n",
        "        \n",
        "        #gcn-dgl\n",
        "        g = G\n",
        "        self.gcn_dgl.train()\n",
        "        logits = self.gcn_dgl(g, lstm_out)\n",
        "        output = F.log_softmax(logits, 1)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "    def _init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_size, device=self.device), \n",
        "            torch.randn(2, 1, self.hidden_size, device=self.device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E160725LCClO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "4c2d5e1a-2134-4464-ea2e-58aef1bd390c"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "import os.path as path\n",
        "import sys\n",
        "import argparse\n",
        "ROOTDIR = path.dirname(os.getcwd())\n",
        "DATADIR = path.join(\"data\")\n",
        "sys.path.append(DATADIR)\n",
        "import utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, g, features, labels):\n",
        "    model.eval()\n",
        "    with th.no_grad():\n",
        "        logits = model(g, features)\n",
        "        labels = labels\n",
        "        _, indices = th.max(logits, dim=1)\n",
        "        correct = th.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "def train(esa, data, label, criterion, optimizer, n_epoch, save_every, directory, device, clip, entity2vec, pred2ix, regularization, adj):\n",
        "    if not path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    print('n epoch', n_epoch)\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i in range(len(data)):\n",
        "            #esa.zero_grad()\n",
        "            pred_tensor, obj_tensor = tensor_from_data(entity2vec, pred2ix, data[i])\n",
        "            input_tensor = [pred_tensor.to(device), obj_tensor.to(device)]\n",
        "            weight_tensor = tensor_from_weight(len(data[i]), data[i], label[i]).to(device)\n",
        "            atten_weight = esa(input_tensor, adj[i])\n",
        "            # loss\n",
        "            if regularization:\n",
        "                loss = criterion(atten_weight.view(-1), weight_tensor.view(-1)).to(device) + \\\n",
        "                    torch.sum(torch.abs(atten_weight))\n",
        "            else:\n",
        "                loss = criterion(atten_weight.view(-1), weight_tensor.view(-1)).to(device)\n",
        "\n",
        "            # clip gradient\n",
        "            _ = nn.utils.clip_grad_norm_(esa.parameters(), clip)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            #acc = evaluate(esa, adj[i], features, labels)\n",
        "            #total_acc += acc\n",
        "        total_loss = total_loss/len(data)\n",
        "        #total_acc = totela_acc/len(data)\n",
        "        if epoch % save_every == 0:\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": esa.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": total_loss\n",
        "                #'acc': total_acc\n",
        "                }, path.join(directory, \"checkpoint_epoch_{}.pt\".format(epoch)))\n",
        "        print(\"epoch: {}\".format(epoch), total_loss)\n",
        "\n",
        "def train_iter(db_name, base, data, label, pred2ix, pred2ix_size, entity2vec, pred_embedding_dim, transe_dim, hidden_size, criterion, clip, lr, n_epoch, save_every, regularization, device, data_graph):\n",
        "    if regularization == True:\n",
        "        print(\"use regularization in training\")\n",
        "    for i in range(5):\n",
        "        train_data, train_label, train_graph, _, _, _ = split_data(base, i, data, label, data_graph)\n",
        "        esa = ESA(pred2ix_size, pred_embedding_dim, transe_dim, hidden_size, device, label)\n",
        "        esa.to(device)\n",
        "        optimizer = optim.Adam(esa.parameters(), lr=lr, amsgrad=False)\n",
        "        directory = os.path.join(os.getcwd(), \"esa_checkpoint-{}-{}\".format(db_name, i))\n",
        "        train(esa, train_data, train_label, criterion, optimizer, n_epoch, save_every, directory, device, clip, entity2vec, pred2ix, regularization, train_graph)\n",
        "\n",
        "def writer(DB_DIR, skip_i, directory, top_or_rank, output):\n",
        "    with open(path.join(DB_DIR, \n",
        "            \"{}\".format(skip_i+1), \n",
        "            \"{}_desc.nt\".format(skip_i+1)),\n",
        "            encoding=\"utf8\") as fin, \\\n",
        "    open(path.join(directory,\n",
        "            \"{}\".format(skip_i+1),\n",
        "            \"{}_{}.nt\".format(skip_i+1, top_or_rank)),\n",
        "            \"w\", encoding=\"utf8\") as fout:\n",
        "        if top_or_rank == \"top5\" or top_or_rank == \"top10\":\n",
        "            top_list = output.squeeze(0).numpy().tolist()\n",
        "            print('top list', top_list)\n",
        "            for t_num, triple in enumerate(fin):\n",
        "                if t_num in top_list:\n",
        "                    fout.write(triple)\n",
        "        elif top_or_rank == \"rank\":\n",
        "            rank_list = output.squeeze(0).numpy().tolist()\n",
        "            #print(rank_list)\n",
        "            triples = [triple for _, triple in enumerate(fin)]\n",
        "            for rank in rank_list:\n",
        "              try:\n",
        "                  fout.write(triples[rank])\n",
        "              except:\n",
        "                  pass\n",
        "                    \n",
        "    return\n",
        "\n",
        "def generator(DB_NAME, base, data, label, entity2vec, pred2ix, pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device, use_epoch, db_base, DB_DIR, skip_num, data_graph):\n",
        "    directory = path.join(\"data_esa\", DB_NAME)\n",
        "    if not path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    print(\"generating entity summarization results:\") \n",
        "    for num in tqdm(range(5)):\n",
        "        CHECK_DIR = path.join(\"esa_checkpoint-{}-{}\".format(DB_NAME, num))\n",
        "        print(\"CHECK_DIR\", CHECK_DIR)\n",
        "        esa = ESA(pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device, label)\n",
        "        checkpoint = torch.load(path.join(CHECK_DIR, \"checkpoint_epoch_{}.pt\".format(use_epoch)))\n",
        "        esa.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        esa.to(device)\n",
        "        for i in range(num*base, (num+1)*base):\n",
        "            print('i', i)\n",
        "            data_i = i - num*base\n",
        "            print('data_i', data_i)\n",
        "            _, _,_, test_data, test_label, test_graph = split_data(base, num, data, label, data_graph)\n",
        "            pred_tensor, obj_tensor = tensor_from_data(entity2vec, pred2ix, test_data[data_i])\n",
        "            input_tensor = [pred_tensor.to(device), obj_tensor.to(device)]\n",
        "            weight_tensor = tensor_from_weight(len(test_data[data_i]), test_data[data_i], test_label[data_i]).to(device)\n",
        "            atten_weight = esa(input_tensor, test_graph[data_i])\n",
        "            atten_weight = atten_weight.view(1, -1).cpu()\n",
        "            weight_tensor = weight_tensor.view(1, -1).cpu()\n",
        "            (_, label_top10) = torch.topk(weight_tensor, 10)\n",
        "            (_, output_top10) = torch.topk(atten_weight, 10)\n",
        "            (_, label_top5) = torch.topk(weight_tensor, 5)\n",
        "            (_, output_top5) = torch.topk(atten_weight, 5)\n",
        "            (_, output_rank) = torch.topk(atten_weight, len(test_data[data_i]))\n",
        "            if num == 4:\n",
        "                skip_i = i + skip_num + db_base\n",
        "            else:\n",
        "                skip_i = i + db_base\n",
        "            if not path.exists(path.join(directory, \"{}\".format(skip_i+1))):\n",
        "                os.makedirs(path.join(directory, \"{}\".format(skip_i+1)))\n",
        "            writer(DB_DIR, skip_i, directory, \"top10\", output_top10)\n",
        "            writer(DB_DIR, skip_i, directory, \"top5\", output_top5)\n",
        "            writer(DB_DIR, skip_i, directory, \"rank\", output_rank)\n",
        "\n",
        "def main(DB_NAME, mode, top_n, file_n, transE_dim, pred_embedding_dim, lr, clip, save_every, n_epoch, use_epoch, loss_function, regularization):\n",
        "  if DB_NAME == \"dbpedia\":\n",
        "    print(\"training model on dbpedia\")\n",
        "    DB_START, DB_END = [1, 141], [101, 166]\n",
        "    base = 25\n",
        "    skip_num = 40\n",
        "    db_base = 0\n",
        "  elif DB_NAME == \"lmdb\":\n",
        "    print(\"training model on lmdb\")\n",
        "    DB_START, DB_END = [101, 166], [141, 176]\n",
        "    base = 10\n",
        "    skip_num = 25\n",
        "    db_base = 100\n",
        "  DB_DIR = path.join(DATADIR, DB_NAME)\n",
        "  print(\"DB_DIR\", DB_DIR)\n",
        "\n",
        "  # load data\n",
        "  data, _, label, _, _, data_graph = process_data(DB_NAME, DB_START, DB_END, top_n, file_n)\n",
        "\n",
        "  entity2vec, pred2vec, entity2ix, pred2ix = load_transE(DB_NAME)\n",
        "  pred2ix_size = len(pred2ix)\n",
        "  hidden_size = transE_dim + pred_embedding_dim\n",
        "\n",
        "  # train\n",
        "  ## cuda \n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(\"cuda or cpu: {}\".format(device))\n",
        "\n",
        "  ## loss function\n",
        "  if loss_function == \"BCE\":\n",
        "    criterion = torch.nn.BCELoss()\n",
        "  elif loss_function == \"MSE\":\n",
        "    criterion = torch.nn.MSELoss()\n",
        "  else:\n",
        "    print(\"please choose choose the correct loss fucntion\")\n",
        "    sys.exit()\n",
        "  print(\"loss function: {}\".format(loss_function))\n",
        "\n",
        "  if mode == \"train\" or mode == \"all\":\n",
        "    ## training iteration (5-fold cross validation)\n",
        "    train_iter(DB_NAME, base, data, label, pred2ix, pred2ix_size, entity2vec, pred_embedding_dim, transE_dim, hidden_size, criterion, clip, lr, n_epoch, save_every, regularization, device, data_graph)\n",
        "\n",
        "  if mode == \"test\" or mode == \"all\":\n",
        "    #generate\n",
        "    generator(DB_NAME, base, data, label, entity2vec, pred2ix, pred2ix_size, pred_embedding_dim, transE_dim, hidden_size, device, use_epoch, db_base, DB_DIR, skip_num, data_graph)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x42OxJVb9PPb",
        "colab_type": "code",
        "outputId": "c3ab92c1-5b6f-4885-9c25-828e52970a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main(\"lmdb\", \"train\", 5, 6, 100,100, 0.0001, 1, 2, 50, 24, \"BCE\", False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model on lmdb\n",
            "DB_DIR data/lmdb\n",
            "['1789']\n",
            "['43']\n",
            "cuda or cpu: cpu\n",
            "loss function: BCE\n",
            "hidden_size 200\n",
            "n epoch 50\n",
            "epoch: 0 0.549297197535634\n",
            "epoch: 1 0.549297197535634\n",
            "epoch: 2 0.549297197535634\n",
            "epoch: 3 0.549297197535634\n",
            "epoch: 4 0.549297197535634\n",
            "epoch: 5 0.549297197535634\n",
            "epoch: 6 0.549297197535634\n",
            "epoch: 7 0.549297197535634\n",
            "epoch: 8 0.549297197535634\n",
            "epoch: 9 0.549297197535634\n",
            "epoch: 10 0.549297197535634\n",
            "epoch: 11 0.549297197535634\n",
            "epoch: 12 0.549297197535634\n",
            "epoch: 13 0.549297197535634\n",
            "epoch: 14 0.549297197535634\n",
            "epoch: 15 0.549297197535634\n",
            "epoch: 16 0.549297197535634\n",
            "epoch: 17 0.549297197535634\n",
            "epoch: 18 0.549297197535634\n",
            "epoch: 19 0.549297197535634\n",
            "epoch: 20 0.549297197535634\n",
            "epoch: 21 0.549297197535634\n",
            "epoch: 22 0.549297197535634\n",
            "epoch: 23 0.549297197535634\n",
            "epoch: 24 0.549297197535634\n",
            "epoch: 25 0.549297197535634\n",
            "epoch: 26 0.549297197535634\n",
            "epoch: 27 0.549297197535634\n",
            "epoch: 28 0.549297197535634\n",
            "epoch: 29 0.549297197535634\n",
            "epoch: 30 0.549297197535634\n",
            "epoch: 31 0.549297197535634\n",
            "epoch: 32 0.549297197535634\n",
            "epoch: 33 0.549297197535634\n",
            "epoch: 34 0.549297197535634\n",
            "epoch: 35 0.549297197535634\n",
            "epoch: 36 0.549297197535634\n",
            "epoch: 37 0.549297197535634\n",
            "epoch: 38 0.549297197535634\n",
            "epoch: 39 0.549297197535634\n",
            "epoch: 40 0.549297197535634\n",
            "epoch: 41 0.549297197535634\n",
            "epoch: 42 0.549297197535634\n",
            "epoch: 43 0.549297197535634\n",
            "epoch: 44 0.549297197535634\n",
            "epoch: 45 0.549297197535634\n",
            "epoch: 46 0.549297197535634\n",
            "epoch: 47 0.549297197535634\n",
            "epoch: 48 0.549297197535634\n",
            "epoch: 49 0.549297197535634\n",
            "hidden_size 200\n",
            "n epoch 50\n",
            "epoch: 0 0.5448080059140921\n",
            "epoch: 1 0.5448080059140921\n",
            "epoch: 2 0.5448080059140921\n",
            "epoch: 3 0.5448080059140921\n",
            "epoch: 4 0.5448080059140921\n",
            "epoch: 5 0.5448080059140921\n",
            "epoch: 6 0.5448080059140921\n",
            "epoch: 7 0.5448080059140921\n",
            "epoch: 8 0.5448080059140921\n",
            "epoch: 9 0.5448080059140921\n",
            "epoch: 10 0.5448080059140921\n",
            "epoch: 11 0.5448080059140921\n",
            "epoch: 12 0.5448080059140921\n",
            "epoch: 13 0.5448080059140921\n",
            "epoch: 14 0.5448080059140921\n",
            "epoch: 15 0.5448080059140921\n",
            "epoch: 16 0.5448080059140921\n",
            "epoch: 17 0.5448080059140921\n",
            "epoch: 18 0.5448080059140921\n",
            "epoch: 19 0.5448080059140921\n",
            "epoch: 20 0.5448080059140921\n",
            "epoch: 21 0.5448080059140921\n",
            "epoch: 22 0.5448080059140921\n",
            "epoch: 23 0.5448080059140921\n",
            "epoch: 24 0.5448080059140921\n",
            "epoch: 25 0.5448080059140921\n",
            "epoch: 26 0.5448080059140921\n",
            "epoch: 27 0.5448080059140921\n",
            "epoch: 28 0.5448080059140921\n",
            "epoch: 29 0.5448080059140921\n",
            "epoch: 30 0.5448080059140921\n",
            "epoch: 31 0.5448080059140921\n",
            "epoch: 32 0.5448080059140921\n",
            "epoch: 33 0.5448080059140921\n",
            "epoch: 34 0.5448080059140921\n",
            "epoch: 35 0.5448080059140921\n",
            "epoch: 36 0.5448080059140921\n",
            "epoch: 37 0.5448080059140921\n",
            "epoch: 38 0.5448080059140921\n",
            "epoch: 39 0.5448080059140921\n",
            "epoch: 40 0.5448080059140921\n",
            "epoch: 41 0.5448080059140921\n",
            "epoch: 42 0.5448080059140921\n",
            "epoch: 43 0.5448080059140921\n",
            "epoch: 44 0.5448080059140921\n",
            "epoch: 45 0.5448080059140921\n",
            "epoch: 46 0.5448080059140921\n",
            "epoch: 47 0.5448080059140921\n",
            "epoch: 48 0.5448080059140921\n",
            "epoch: 49 0.5448080059140921\n",
            "hidden_size 200\n",
            "n epoch 50\n",
            "epoch: 0 0.6231381151825189\n",
            "epoch: 1 0.6231381151825189\n",
            "epoch: 2 0.6231381151825189\n",
            "epoch: 3 0.6231381151825189\n",
            "epoch: 4 0.6231381151825189\n",
            "epoch: 5 0.6231381151825189\n",
            "epoch: 6 0.6231381151825189\n",
            "epoch: 7 0.6231381151825189\n",
            "epoch: 8 0.6231381151825189\n",
            "epoch: 9 0.6231381151825189\n",
            "epoch: 10 0.6231381151825189\n",
            "epoch: 11 0.6231381151825189\n",
            "epoch: 12 0.6231381151825189\n",
            "epoch: 13 0.6231381151825189\n",
            "epoch: 14 0.6231381151825189\n",
            "epoch: 15 0.6231381151825189\n",
            "epoch: 16 0.6231381151825189\n",
            "epoch: 17 0.6231381151825189\n",
            "epoch: 18 0.6231381151825189\n",
            "epoch: 19 0.6231381151825189\n",
            "epoch: 20 0.6231381151825189\n",
            "epoch: 21 0.6231381151825189\n",
            "epoch: 22 0.6231381151825189\n",
            "epoch: 23 0.6231381151825189\n",
            "epoch: 24 0.6231381151825189\n",
            "epoch: 25 0.6231381151825189\n",
            "epoch: 26 0.6231381151825189\n",
            "epoch: 27 0.6231381151825189\n",
            "epoch: 28 0.6231381151825189\n",
            "epoch: 29 0.6231381151825189\n",
            "epoch: 30 0.6231381151825189\n",
            "epoch: 31 0.6231381151825189\n",
            "epoch: 32 0.6231381151825189\n",
            "epoch: 33 0.6231381151825189\n",
            "epoch: 34 0.6231381151825189\n",
            "epoch: 35 0.6231381151825189\n",
            "epoch: 36 0.6231381151825189\n",
            "epoch: 37 0.6231381151825189\n",
            "epoch: 38 0.6231381151825189\n",
            "epoch: 39 0.6231381151825189\n",
            "epoch: 40 0.6231381151825189\n",
            "epoch: 41 0.6231381151825189\n",
            "epoch: 42 0.6231381151825189\n",
            "epoch: 43 0.6231381151825189\n",
            "epoch: 44 0.6231381151825189\n",
            "epoch: 45 0.6231381151825189\n",
            "epoch: 46 0.6231381151825189\n",
            "epoch: 47 0.6231381151825189\n",
            "epoch: 48 0.6231381151825189\n",
            "epoch: 49 0.6231381151825189\n",
            "hidden_size 200\n",
            "n epoch 50\n",
            "epoch: 0 0.6215054638683796\n",
            "epoch: 1 0.6215054638683796\n",
            "epoch: 2 0.6215054638683796\n",
            "epoch: 3 0.6215054638683796\n",
            "epoch: 4 0.6215054638683796\n",
            "epoch: 5 0.6215054638683796\n",
            "epoch: 6 0.6215054638683796\n",
            "epoch: 7 0.6215054638683796\n",
            "epoch: 8 0.6215054638683796\n",
            "epoch: 9 0.6215054638683796\n",
            "epoch: 10 0.6215054638683796\n",
            "epoch: 11 0.6215054638683796\n",
            "epoch: 12 0.6215054638683796\n",
            "epoch: 13 0.6215054638683796\n",
            "epoch: 14 0.6215054638683796\n",
            "epoch: 15 0.6215054638683796\n",
            "epoch: 16 0.6215054638683796\n",
            "epoch: 17 0.6215054638683796\n",
            "epoch: 18 0.6215054638683796\n",
            "epoch: 19 0.6215054638683796\n",
            "epoch: 20 0.6215054638683796\n",
            "epoch: 21 0.6215054638683796\n",
            "epoch: 22 0.6215054638683796\n",
            "epoch: 23 0.6215054638683796\n",
            "epoch: 24 0.6215054638683796\n",
            "epoch: 25 0.6215054638683796\n",
            "epoch: 26 0.6215054638683796\n",
            "epoch: 27 0.6215054638683796\n",
            "epoch: 28 0.6215054638683796\n",
            "epoch: 29 0.6215054638683796\n",
            "epoch: 30 0.6215054638683796\n",
            "epoch: 31 0.6215054638683796\n",
            "epoch: 32 0.6215054638683796\n",
            "epoch: 33 0.6215054638683796\n",
            "epoch: 34 0.6215054638683796\n",
            "epoch: 35 0.6215054638683796\n",
            "epoch: 36 0.6215054638683796\n",
            "epoch: 37 0.6215054638683796\n",
            "epoch: 38 0.6215054638683796\n",
            "epoch: 39 0.6215054638683796\n",
            "epoch: 40 0.6215054638683796\n",
            "epoch: 41 0.6215054638683796\n",
            "epoch: 42 0.6215054638683796\n",
            "epoch: 43 0.6215054638683796\n",
            "epoch: 44 0.6215054638683796\n",
            "epoch: 45 0.6215054638683796\n",
            "epoch: 46 0.6215054638683796\n",
            "epoch: 47 0.6215054638683796\n",
            "epoch: 48 0.6215054638683796\n",
            "epoch: 49 0.6215054638683796\n",
            "hidden_size 200\n",
            "n epoch 50\n",
            "epoch: 0 0.5846762824803591\n",
            "epoch: 1 0.5846762824803591\n",
            "epoch: 2 0.5846762824803591\n",
            "epoch: 3 0.5846762824803591\n",
            "epoch: 4 0.5846762824803591\n",
            "epoch: 5 0.5846762824803591\n",
            "epoch: 6 0.5846762824803591\n",
            "epoch: 7 0.5846762824803591\n",
            "epoch: 8 0.5846762824803591\n",
            "epoch: 9 0.5846762824803591\n",
            "epoch: 10 0.5846762824803591\n",
            "epoch: 11 0.5846762824803591\n",
            "epoch: 12 0.5846762824803591\n",
            "epoch: 13 0.5846762824803591\n",
            "epoch: 14 0.5846762824803591\n",
            "epoch: 15 0.5846762824803591\n",
            "epoch: 16 0.5846762824803591\n",
            "epoch: 17 0.5846762824803591\n",
            "epoch: 18 0.5846762824803591\n",
            "epoch: 19 0.5846762824803591\n",
            "epoch: 20 0.5846762824803591\n",
            "epoch: 21 0.5846762824803591\n",
            "epoch: 22 0.5846762824803591\n",
            "epoch: 23 0.5846762824803591\n",
            "epoch: 24 0.5846762824803591\n",
            "epoch: 25 0.5846762824803591\n",
            "epoch: 26 0.5846762824803591\n",
            "epoch: 27 0.5846762824803591\n",
            "epoch: 28 0.5846762824803591\n",
            "epoch: 29 0.5846762824803591\n",
            "epoch: 30 0.5846762824803591\n",
            "epoch: 31 0.5846762824803591\n",
            "epoch: 32 0.5846762824803591\n",
            "epoch: 33 0.5846762824803591\n",
            "epoch: 34 0.5846762824803591\n",
            "epoch: 35 0.5846762824803591\n",
            "epoch: 36 0.5846762824803591\n",
            "epoch: 37 0.5846762824803591\n",
            "epoch: 38 0.5846762824803591\n",
            "epoch: 39 0.5846762824803591\n",
            "epoch: 40 0.5846762824803591\n",
            "epoch: 41 0.5846762824803591\n",
            "epoch: 42 0.5846762824803591\n",
            "epoch: 43 0.5846762824803591\n",
            "epoch: 44 0.5846762824803591\n",
            "epoch: 45 0.5846762824803591\n",
            "epoch: 46 0.5846762824803591\n",
            "epoch: 47 0.5846762824803591\n",
            "epoch: 48 0.5846762824803591\n",
            "epoch: 49 0.5846762824803591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IaDuyj3WuPf",
        "colab_type": "code",
        "outputId": "3bf3a48b-8cd0-4b62-a3fb-1575fea8206b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main(\"lmdb\", \"test\", 5, 6, 100,100, 0.0001, 1, 2, 50, 24, \"BCE\", False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model on lmdb\n",
            "DB_DIR data/lmdb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['1789']\n",
            "['43']\n",
            "cuda or cpu: cpu\n",
            "loss function: BCE\n",
            "generating entity summarization results:\n",
            "CHECK_DIR esa_checkpoint-lmdb-0\n",
            "hidden_size 200\n",
            "i 0\n",
            "data_i 0\n",
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n",
            "i 1\n",
            "data_i 1\n",
            "top list [27, 19, 20, 21, 22, 23, 24, 25, 26, 34]\n",
            "top list [25, 23, 24, 22, 26]\n",
            "i 2\n",
            "data_i 2\n",
            "top list [25, 17, 18, 19, 20, 21, 22, 23, 24, 31]\n",
            "top list [23, 21, 22, 20, 24]\n",
            "i 3\n",
            "data_i 3\n",
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n",
            "i 4\n",
            "data_i 4\n",
            "top list [30, 35, 34, 33, 32, 31, 27, 25, 26, 36]\n",
            "top list [33, 31, 32, 30, 34]\n",
            "i 5\n",
            "data_i 5\n",
            "top list [36, 43, 42, 41, 40, 39, 38, 37, 31, 32]\n",
            "top list [40, 37, 38, 39, 42]\n",
            "i 6\n",
            "data_i 6\n",
            "top list [24, 28, 27, 26, 25, 22, 20, 21, 29, 23]\n",
            "top list [27, 25, 26, 24, 28]\n",
            "i 7\n",
            "data_i 7\n",
            "top list [30, 35, 34, 33, 32, 31, 27, 25, 26, 36]\n",
            "top list [33, 31, 32, 30, 34]\n",
            "i 8\n",
            "data_i 8\n",
            "top list [19, 13, 14, 15, 16, 17, 18, 22, 24, 23]\n",
            "top list [15, 18, 17, 16, 19]\n",
            "i 9\n",
            "data_i 9\n",
            "top list [20, 14, 15, 16, 17, 18, 19, 25, 24, 23]\n",
            "top list [16, 19, 18, 17, 20]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:00<00:01,  2.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CHECK_DIR esa_checkpoint-lmdb-1\n",
            "hidden_size 200\n",
            "i 10\n",
            "data_i 0\n",
            "top list [23, 16, 17, 18, 19, 20, 21, 22, 28, 27]\n",
            "top list [19, 22, 21, 20, 17]\n",
            "i 11\n",
            "data_i 1\n",
            "top list [26, 31, 30, 29, 28, 27, 24, 22, 23, 32]\n",
            "top list [29, 27, 28, 26, 30]\n",
            "i 12\n",
            "data_i 2\n",
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n",
            "i 13\n",
            "data_i 3\n",
            "top list [27, 19, 20, 21, 22, 23, 24, 25, 26, 34]\n",
            "top list [25, 23, 24, 22, 26]\n",
            "i 14\n",
            "data_i 4\n",
            "top list [35, 42, 41, 40, 39, 38, 37, 36, 30, 31]\n",
            "top list [39, 36, 37, 38, 41]\n",
            "i 15\n",
            "data_i 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [00:00<00:01,  2.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top list [24, 17, 18, 19, 20, 21, 22, 23, 30, 29]\n",
            "top list [20, 23, 22, 21, 18]\n",
            "i 16\n",
            "data_i 6\n",
            "top list [20, 14, 15, 16, 17, 18, 19, 25, 24, 23]\n",
            "top list [16, 19, 18, 17, 20]\n",
            "i 17\n",
            "data_i 7\n",
            "top list [24, 28, 27, 26, 25, 22, 20, 21, 29, 23]\n",
            "top list [27, 25, 26, 24, 28]\n",
            "i 18\n",
            "data_i 8\n",
            "top list [23, 27, 26, 25, 24, 21, 19, 20, 28, 22]\n",
            "top list [26, 24, 25, 23, 27]\n",
            "i 19\n",
            "data_i 9\n",
            "top list [25, 29, 28, 27, 26, 23, 21, 22, 30, 24]\n",
            "top list [28, 26, 27, 25, 29]\n",
            "CHECK_DIR esa_checkpoint-lmdb-2\n",
            "hidden_size 200\n",
            "i 20\n",
            "data_i 0\n",
            "top list [38, 45, 44, 43, 42, 41, 40, 39, 32, 33]\n",
            "top list [42, 39, 40, 41, 44]\n",
            "i 21\n",
            "data_i 1\n",
            "top list [37, 44, 43, 42, 41, 40, 39, 38, 32, 33]\n",
            "top list [41, 38, 39, 40, 43]\n",
            "i 22\n",
            "data_i 2\n",
            "top list [78, 71, 72, 73, 74, 75, 76, 77, 83, 82]\n",
            "top list [74, 77, 76, 75, 72]\n",
            "i 23\n",
            "data_i 3\n",
            "top list [45, 53, 52, 51, 50, 49, 48, 47, 46, 38]\n",
            "top list [47, 49, 48, 50, 46]\n",
            "i 24\n",
            "data_i 4\n",
            "top list [44, 52, 51, 50, 49, 48, 47, 46, 45, 37]\n",
            "top list [46, 48, 47, 49, 45]\n",
            "i 25\n",
            "data_i 5\n",
            "top list [30, 35, 34, 33, 32, 31, 27, 25, 26, 36]\n",
            "top list [33, 31, 32, 30, 34]\n",
            "i 26\n",
            "data_i 6\n",
            "top list [52, 48, 49, 50, 51, 54, 56, 55, 47, 53]\n",
            "top list [49, 51, 50, 52, 48]\n",
            "i 27\n",
            "data_i 7\n",
            "top list [31, 37, 36, 35, 34, 33, 32, 28, 26, 27]\n",
            "top list [35, 32, 33, 34, 31]\n",
            "i 28\n",
            "data_i 8\n",
            "top list [60, 55, 56, 57, 58, 59, 62, 64, 63, 54]\n",
            "top list [57, 59, 58, 60, 56]\n",
            "i 29\n",
            "data_i 9\n",
            "top list [30, 35, 34, 33, 32, 31, 27, 25, 26, 36]\n",
            "top list "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [00:01<00:00,  2.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[33, 31, 32, 30, 34]\n",
            "CHECK_DIR esa_checkpoint-lmdb-3\n",
            "hidden_size 200\n",
            "i 30\n",
            "data_i 0\n",
            "top list [51, 47, 48, 49, 50, 53, 55, 54, 46, 52]\n",
            "top list [48, 50, 49, 51, 47]\n",
            "i 31\n",
            "data_i 1\n",
            "top list [33, 39, 38, 37, 36, 35, 34, 28, 29, 30]\n",
            "top list [37, 34, 35, 36, 33]\n",
            "i 32\n",
            "data_i 2\n",
            "top list [51, 47, 48, 49, 50, 53, 55, 54, 46, 52]\n",
            "top list [48, 50, 49, 51, 47]\n",
            "i 33\n",
            "data_i 3\n",
            "top list [36, 43, 42, 41, 40, 39, 38, 37, 31, 32]\n",
            "top list [40, 37, 38, 39, 42]\n",
            "i 34\n",
            "data_i 4\n",
            "top list [40, 47, 46, 45, 44, 43, 42, 41, 34, 35]\n",
            "top list [44, 41, 42, 43, 46]\n",
            "i 35\n",
            "data_i 5\n",
            "top list [209, 205, 206, 207, 208, 211, 213, 212, 204, 210]\n",
            "top list [206, 208, 207, 209, 205]\n",
            "i 36\n",
            "data_i 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [00:01<00:00,  2.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n",
            "i 37\n",
            "data_i 7\n",
            "top list [38, 45, 44, 43, 42, 41, 40, 39, 32, 33]\n",
            "top list [42, 39, 40, 41, 44]\n",
            "i 38\n",
            "data_i 8\n",
            "top list [40, 47, 46, 45, 44, 43, 42, 41, 34, 35]\n",
            "top list [44, 41, 42, 43, 46]\n",
            "i 39\n",
            "data_i 9\n",
            "top list [30, 36, 35, 34, 33, 32, 31, 27, 25, 26]\n",
            "top list [34, 31, 32, 33, 30]\n",
            "CHECK_DIR esa_checkpoint-lmdb-4\n",
            "hidden_size 200\n",
            "i 40\n",
            "data_i 0\n",
            "top list [34, 40, 39, 38, 37, 36, 35, 29, 30, 31]\n",
            "top list [38, 35, 36, 37, 34]\n",
            "i 41\n",
            "data_i 1\n",
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n",
            "i 42\n",
            "data_i 2\n",
            "top list [41, 49, 48, 47, 46, 45, 44, 43, 42, 35]\n",
            "top list [43, 45, 44, 46, 42]\n",
            "i 43\n",
            "data_i 3\n",
            "top list [23, 16, 17, 18, 19, 20, 21, 22, 28, 27]\n",
            "top list [19, 22, 21, 20, 17]\n",
            "i 44\n",
            "data_i 4\n",
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n",
            "i 45\n",
            "data_i 5\n",
            "top list [28, 33, 32, 31, 30, 29, 25, 23, 24, 34]\n",
            "top list [31, 29, 30, 28, 32]\n",
            "i 46\n",
            "data_i 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 5/5 [00:02<00:00,  2.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "top list [40, 47, 46, 45, 44, 43, 42, 41, 34, 35]\n",
            "top list [44, 41, 42, 43, 46]\n",
            "i 47\n",
            "data_i 7\n",
            "top list [28, 33, 32, 31, 30, 29, 25, 23, 24, 34]\n",
            "top list [31, 29, 30, 28, 32]\n",
            "i 48\n",
            "data_i 8\n",
            "top list [28, 33, 32, 31, 30, 29, 25, 23, 24, 34]\n",
            "top list [31, 29, 30, 28, 32]\n",
            "i 49\n",
            "data_i 9\n",
            "top list [29, 34, 33, 32, 31, 30, 26, 24, 25, 35]\n",
            "top list [32, 30, 31, 29, 33]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9BQYysIpUH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}